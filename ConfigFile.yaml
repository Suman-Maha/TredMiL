# logger options
image_save_iter: 100         # How often do you want to save output images during training
image_display_iter: 100       # How often do you want to display output images during training
display_size: 1               # How many images do you want to display each time
snapshot_save_iter: 100     # How often do you want to save trained models
log_iter: 1                   # How often do you want to log the training stats

# optimization options
max_iter: 10000             # maximum number of training iterations
batch_size: 1                 # batch size
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialization [gaussian/ kaiming/ xavier/ orthogonal]
lr: 0.0001                    # initial learning rate
lr_policy: step               # learning rate scheduler
step_size: 1000             # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate
gan_w: 1                      # weight of adversarial loss
recon_w: 10                 # weight of image reconstruction loss
recon_c_a_w: 1                  # weight of style reconstruction loss
recon_s_d_w: 1                  # weight of content reconstruction loss
#recon_x_cyc_w: 0              # weight of explicit style augmented cycle consistency loss
#vgg_w: 0                      # weight of domain-invariant perceptual loss
regularization_w: 1           # weight of the regularization parameter through KL divergence

# model options
gen:
  dim: 64                     # number of filters in the bottommost layer
  mlp_dim: 256                # number of filters in MLP
  color_appearance_dim: 20                 # length of color code
  num_components: 2           # number of truncated normal mixture components 
  activ: relu                 # activation function [relu/ lrelu/ prelu/ selu/ tanh]
  num_down: 2             # number of downsampling layers in content encoder
  num_down_block: 4             # number of downsampling layers in content encoder
  num_res: 4                    # number of residual blocks in content encoder/ decoder
  pad_type: reflect           # padding type [zero/ reflect]
disc:
  dim: 64                     # number of filters in the bottommost layer
  norm: none                  # normalization layer [none/ bn/ in/ ln]
  activ: lrelu                # activation function [relu/ lrelu/ prelu/ selu/ tanh]
  num_layers: 4                  # number of layers in D
  gan_type: lsgan             # GAN loss [lsgan/ nsgan]
  num_scales: 3               # number of scales
  pad_type: reflect           # padding type [zero/ reflect]

# data options
input_dim: 3                              # number of image channels [1/ 3]
num_workers: 8                              # number of data loading threads
new_size: 256                               # first resize the shortest image side to this size
crop_image_height: 256                      # random crop image of this height
crop_image_width: 256                       # random crop image of this width
train_route: ./DataSet/images_train/     # dataset training data folder location
template_route: ./DataSet/images_template/     # template image patch folder location
source_route: ./DataSet/images_source/     # source image patch folder location
#source_route: ./DataSet/images_source_dust/  # source image patch folder location
validation_route: ./DataSet/images_validation/    # validation image patch folder location

# latent space options
H_stain_mean: -1.0               # center/mean of the latent space representation of Hematoxylin stain
E_stain_mean: 1.0                # center/mean of the latent space representation of Eosin stain
scale_std: 1.0                 # scale or spread of the mixture model (latent space data distribution)
spread_factor: 2.0             # factor k which specifies the spread after which the density is truncated





